\chapter{Výsledky numerických testů}

\section{Testovací data}
V numerických pokusech byla použita data z běhu optimalizačního programu LPopt pro 15. cyklus 1. bloku jaderné elektrárny Temelín (\verb|u1c15|). 
Vsázky byly generované programem LPopt, je zachycen celý optimalizační proces. Data tedy nejsou náhodně vzorkovaná, hodnoty jsou vychýlené 
(důsledek cíle LPopt -- minimalizace \ac{fdh} při současné maximalizaci \ac{rc1}. Soubor obsahoval 
$10^6$ vsázek (přiřazení palivového souboru a jeho rotace ke každé pozici v AZ), vybrané odezvy 
(veličiny \ac{bc}, \ac{fdh}, \ac{fha}, \ac{rc1},\ac{pbu}, \ac{mtc}) 
vypočtené neutronickým kódem Andrea 2D (méně přesný, ale pro účely optimalizace postačující --  používá se kvůli zrychlení výpočtu -- $\approx 0,3$ s na 
vsázku při jednom CPU jádře oproti 30--60 s v 3D režimu) a fyzikální parametry všech uvažovaných palivových souborů -- 121 veličin 
pro každý -- zahrnující informace 
o složení (množství důležitých izotopů), použití (vyhoření, výkon) a vypočtené účinné průřezy (pro jednu a dvě grupy) -- bližší popis je 
v příloze~\ref{app:params}.

Dataset tvořila matice $\mathcal{C}$ rozměru $(10^6, 56)$, jejíž řádky obsahovaly vektory $\bv{c}$ (definovány dle \ref{ch:repr}) jednotlivých 
vsázek. Odezvy byly v matici $\mathcal{R}$ rozměru $(10^6, 6)$, jejíž řádky odpovídaly odezvám \ac{az} s konfigurací popsanou příslušným 
řádkem v $\mathcal{C}$. 
Dále byla přiložena tabulka \ac{fap} rozměru (150, 121), jejíž řádky obsahovaly fyzikální parametry palivových souborů v jednotlivých rotacích. 
Matice $\bv{P}$ se vytvořila přiřazením řádků tabulky s palivem ke složkám $\bv{c}$.

K dispozici bylo 25 palivových souborů s parametry vypočtenými pro všechny rotace (tj. celkem $150 = 25\cdot6$ různých \ac{ps}). Seznam 
fyzikálních parametrů, popisujících palivové soubory, 
je v příloze \ref{app:params}. 
%Veličiny tvořící výstup z neutronického kódu jsou popsány níže. 
Odezvy jsou veličiny vyhodnocované 
v průběhu optimalizačního programu LPopt -- koncentraci \jdt{H_3BO_3} EOC, koeficient poproutkového nevyrovnání \ac{fdh}, 
koeficient pokazetového nevyrovnání \ac{fha} ($=K_q$), parametr \ac{rc1}, maximální poproutkové vyhoření (\ac{pbu}) a hodnotu \ac{mtc}.

\section{Aproximace jednovrstvou neuronovou sítí}
První pokus o aproximaci byl implementován v MATLAB Statistics and Machine Learning Toolboxu metodou Shallow Neural Network. 
První pokusy byly prováděny pro relativně malý počet vstupních dat (řádově tisíce dvojic), přičemž výpočet byl proveden na 
PC\footnotei{.}{Notebook, Intel Core i5-3210M, 4GB RAM.} Vstupem byly 28-dimenzionální vektory dle \eqref{eq:fa-norm}, výstupem 
velikost rozdílu odezev. Korelační diagram je na obr.~\ref{mtl_ann.png}. 

Prvním nedostatkem metody byla rychlost tréninku. Pro malé množství vstupů byla přijatelná, ale ve výhledu (plán trénovat pro $\approx 10^5$ dvojic nebo více) 
ne. Problém je snadno řešitelný prováděním výpočtů na výkonném serveru a optimalizací kódu\footnotei{.}{Zahrnuje např. využití parelelizace 
% (v případě MATLABu balíčku \ti{Parallel Computing Toolbox} 
a lepší využití vektorizovaných operací.} 
\pic{mtl_ann.png}{Graf z pokusů s \acs{ann}.}{Graf z pokusů s \acs{ann}.}

Z numerických testů vyplynulo, že při používání uvedených norem dochází k značným chybám. 
Jde vidět, že 
úzkému intervalu vzdáleností odezev přísluší dlouhý interval vzdáleností konfigurací. To mohlo být způsobeno tím, že pro vyhodnocení vzdáleností 
bylo použito málo informací. Dle norem byly vsázky vzdálené, avšak norma zavedená vztahem \eqref{eq:fa-norm} nezohledňuje různé jednotky a škály 
jednotlivých veličin. K odstranění problému by mohlo být vhodné místo norem používat přímo jednotlivé parametry \ac{ps}.

Různé škálování vstupních veličin (popsány v příloze~\ref{app:params}) mohlo vést k situaci, kdy normu prakticky určuje pouze jedna veličina, 
která dosahuje numerických hodnot vyšších, než ostatní. 
To souhlasí s faktem, že vstupní veličiny se liší až o $>30$ řádů (např. mikroskopické účinné průřezy vs. vyhoření). To lze vyřešit vhodným škálováním, 
tj. před zahájením tvorby modelu se vstupy i výstupy lineárně transformují tak, aby všechny veličiny pokrývaly stejné škály. 

Dále je vidět, že predikce byly přesné jen pro blízké vsázky, pro vzdálenější byla predikce značně podhodnocena. To může být způsobeno nedostatečným 
množstvím vstupních dat (tj. model se naučil predikovat podle špatných parametrů, popř. neměl dost dat, aby odhalil správné závislosti). Taktéž mohlo jít 
o použití nedostatečně robusní aproximační metody -- to se však (s ohledem na teorém o univerzálním aproximátoru \cite{hornik} a množství úspěšných aplikací, 
včetně neutronického kódu v \cite{schlunz, erdogan, jiang}) jeví jako nepravděpodobné. Aproximační schopnost \ac{ann} je možno zlepšit větším počtem 
neuronů, to však zpomaluje predikci a může způsobovat větší overfitting modelu. S ohledem na zmíněné nedostatky se v této práci s jednovrstvými sítěmi 
dále nepracuje. 
% Použitá metoda vykazovala následující nedostatky.
% \cen{
% 	\item Pomalý trénink -- čas tvorby modelu byl přijatelný pro $\approx 10^2 - 10^3$ dvojic trénovacích dat. Na reálnou aplikaci bylo potřeba víc.
% 	\item Model nebyl dostatečně přesný. Z provedených pokusů lze odhadovat, že přesnost by mohla být dosažena velmi velký počet neuronů, což by vedlo 
% 		k příliš dlouhému procesu predikce.
% 	\item Jednotlivé veličiny mají různé škálování a to pokrývá velký rozsah (>30 řádů).
% 	\item Zvolená norma nezohledňuje citlivost neutronického kódu na různé veličiny. 
% 	}
%Napočtené body jsou v grafu~\ref{fig:mtl_ann.png}. 

\section{Aproximace metodou regression tree ensambles}
Metoda by vzhledem k zcela jiné podstatě mohla být robusnější, čemuž nasvědčovaly i první pokusy (např. graf~\ref{regtree_old.png}). 
Implementace byla provedena v MATLAB Statistics and Machine Learning Toolboxu. 
\dpic{regtree_old.png}{regtree_test_old.png}{Regression tree ensambles -- test na známých datech.}{Regression tree ensambles -- test na známých datech. Použito 4900 dvojic, vstupní hodnoty 28 norem dle \eqref{eq:fa-norm}, výstup norma rozdílu odezev. Metoda bagged trees, 30 stromů s minimálně 8 prvky na list. Doba tréninku $\sim 9$ s, predikce $\sim 40$ $\upmu$s na vsázku.}{Regression tree ensambles -- test na neznámých datech.}{Regression tree ensambles -- test na neznámých datech. Jde vidět, že změna metody problém s nepřesnou predikcí nezlepšila.}

Bylo přidáno větší množství vstupních dat -- výsledek je v grafu~\ref{regtree.png}. Kvalita predikcí se příliš nezlepšila, výrazně však rostla výpočetní 
náročnost. Pro dataset $>10^6$ dvojic byl čas řádově hodiny na výpočetním clusteru\footnotei{,}{8 jader AMD EPYC 7281, 64 GB RAM} při $>10^8$ dvojicích výpočet 
nedoběhl ani za 12 hodin. 
% 
% Nedostatek spočívá v dlouhém čase tréninku (pro Y dvojic řádově hodiny na PC, prot Z dvojic i > 10 hodin na výpočetním clusteru). 
% 
% Další problém je nedostatečná přesnost predikce -- zejména veličina PBU vykazovala patologické chování (korelace na obr. \ref{regtree.png}). 
% Toto chování se s velikostí modelu příliš nezlepšovalo. 


Možností pro lepší zvládnutí velkého objemu dat je tzv.~\uv{online learning}, kdy je model vytvořen na malé podmnožině datasetu a následně je 
pomocí dalších dat, případně je zpřesňován za běhu. Takový způsob je rychlejší, protože při zpřesňování se pouze upravují některé parametry, ne 
celý model. Navíc umožňuje trénink i na datech, která se nevejdou do paměti nebo přidávání nově získaných dat do již existujícího modelu. 
Nic takového však pro regression tree ensambles není možné. 
Strom optimalizovaný dle určitého datasetu nemusí být vhodný pro nová data, která (vzhledem k velikosti 
konfiguračního prostoru diskutovaném v \ref{sec:diskretnost}) mohou být zcela jiná. To by nebyl problém, pokud by \uv{online learning} byl možný -- existující 
model by se pouze \uv{doladil}. To však nejde -- s ohledem na metodu tvorby stromu by doladění mohlo znamenat opakovat celý proces tréninku, což 
je pro praktické používání nevhodné -- zejména s ohledem na dataset rostoucí v čase by se výpočetní a paměťová náročnost mohla dostat do nepoužitelných 
hodnot\footnotei{.}{Pro omezení paměťové náročnosti existují nástroje -- např. Dask, Vaex. Výpočetní náročnost má omezení zdola.}
\pic{regtree.png}{Korelační diagram z fitu metodou regression tree ensambles}{Korelační diagram z fitu metodou \ti{regression tree ensambles} pro 28 vstupů a jeden výstup. Trénovací množina 490000, testovací 700.}

Přesnost by mohla být zlepšena používáním fyzikálních parametrů \ac{ps} jako vstupů. Těch by pak bylo výrazně víc než 28, což by proces tréninku 
výrazně prodlužovalo. 

S ohledem na výše popsané nedostatky se použití metody regression tree ensambles ukázalo nevhodné.
%% hodit sem obr. pbu_regtree

\section{Aproximace metodou \acs{hpelm}}
Použití HPELM k hledání metriky vyřešilo problém s rychlostí -- trénink na $\approx 8\cdot10^5$ trénovacích dvojicích 
zabral řádově minuty až desítky minut\footnotei{,}{Výkon se značně lišil v závislosti na tom, zda bylo použito GPU -- hardwarová konfigurace 8 jader Intel XEON Gold 6130, 64 GB RAM, 1x nVidia Tesla V100 (16 GB vlastní HBM2 paměti).} predikce jedné vsázky $\approx 40 \mathrm{\upmu s}$ (při hromadné predikci $\approx 10^5$ vsázek).
%% tu bude jeste neco o regtree
%% tu neco o hpelm, tf

Vzhledem k nedostatkům předchozích metod bylo několik procedur změněno. 

\subsection{Škálování}
Veličiny v původním formátu pokrývají > 30 řádů (např. výtěžek $\ce{^{135}Xe}\approx 10^{-28}$, naopak vyhoření souboru $\approx 10^4$). To jednak klade větší 
nároky na výkon (nutno pracovat s větším datovým typem \verb|float64|), jednak zkresluje normy (výtěžek a podobně malé veličniny nemají na normu prakticky žádný 
vliv). Aby se vyrovnal vliv jednotlivých veličin, je dobré provést normalizaci
\eq{
	p_{\cdot,j}^{\ast} = \frac{p_{\cdot,j} - p_{\cdot,j}^{max}}{p_{\cdot,j}^{max} - p_{\cdot,j}^{min}}
	\label{eq:normalizace}
}
případně standardizaci
\eq{
	p_{\cdot,j}^{\ast} = \frac{p_{\cdot,j} - \bar{p_{\cdot,j}}}{\sigma(p_{\cdot,j})}
	\label{eq:standardizace}
}
Obě transformace jsou lineární, první zobrazuje data na interval <0,1>, druhá převádí soubor hodnot tak, aby $\bar{x}=0$ a $\sigma(x)=1$. Díky 
tomu budou mít data menší amplitudy a tedy bude stačit počítat v menším a rychlejším datovém typu (\verb|float32|, případně \verb|float16|). 

Normalizace nebo standardizace byla provedena na vstupních datech vždy před rozdělením dat na trénovací a testovací podmnožinu. 

\subsection{Integrální hodnoty}
Experimenty ukázaly, že model, jehož vstupní data tvoří $N$tice norem jednotlivých pozic v \ac{az} nefunguje -- a to ani v případě, kdy jsou použity 
veličiny transformované dle \eqref{eq:normalizace} nebo \eqref{eq:standardizace}. Důvodem je, že integrální hodnoty nezohledňují velikosti dílčích 
veličin. To je problém zejména tehdy, kdy některé veličiny mohou být protichůdné -- např. množství uranu a množství gadolinia. Dva palivové 
soubory tak mohou mít stejné normy, ale zcela odlišné vlastnosti. Tyto vlastnosti model z norem nemůže poznat a vzniká množství chyb. Vzhledem 
k počtu parametrů ani sofistikované vážení nemusí dávat dobrý smysl -- vstup prostě nebude jednoznačný. Normu palivových souborů by bylo 
potřeba změnit -- takové hledání však je netriviální. 

Problém hledání vhodných norem je možné obejít. Jako vstup se zvolí absolutní hodnoty rozdílu všech veličin na všech pozicích. 
To je možno popsat maticí $\bv{P}\in(\mathbb{R}_0^{+})^{P,N}$. 
Pro účely tvorby modelu je vhodné rozepsat ji do vektoru. Ten je ve tvaru 
\eq{
	\left[\bv{p}_{\cdot,1}^T ,\bv{p}_{\cdot,2}^T,\dots,\bv{p}_{\cdot,N}^T \right]\,.
	\label{eq:Pvec}
}
Data pak lze účelně zapsat ve tvaru matic vstupů a výstupů, kde jednotlivé řádky představují dvojice vstup-výstup. Model je 
pak schopný najít vhodnou kombinaci vstupů \uv{sám}. Po nafitování modelu existují techniky k citlivostní analýze, kdy se odstraní závislosti, které na 
funkci nemají vliv a celý model se zjednoduší. 

\subsection{Velké množství dvojic}
Důvodem špatné predikční schopnosti modelu mohl být počet možných dvojic. Dostupný dataset obsahoval $10^6$ párů konfigurace vsázky -- odezvy. 
Tvorba modelu pro odhad vzdálenosti byla založena na tvorbě všech dvojic z daných dat, kterých je $\approx 5\cdot 10^{11}$. 
Trénování na malé podmnožině prostoru parametrů v případě \ac{ml}-metod nemusí být problém (\cite{schlunz} zmiňuje tvobu modelu z 20~000 dat, což je velmi malá část prostoru parametrů -- důležité je, aby distribuce dat v množině nebyla příliš vychýlená a byla nějak 
\uv{reprezentativní}). 
Model 
natrénovaný na malé podmnožině (v této práci $< 10^7$ dvojic) nevedl ke správným predikcím. 

Trénink modelu na všech dvojicích je možný, ale implementace by byla výrazně složitější. Konkrétně by vyžadovala implementaci 
algoritmu, který bude generovat všechny dvojice v náhodném pořadí\footnotei{.}{Literatura uvádí, že existence uspořádání 
v tréninkových datech je nežádoucí.} Vzhledem k paměťové náročnosti takového úkolu by bylo zapotřebí použít out-of-memory 
metod a vysoké konverze ukládání dat, případně vybrání výrazně menší podmnožiny parametrů palivových souborů pro model, 
případně tvorbě dvojic až v průběhu tréninku. Samotný proces tréninku by pak mohl zabrat i několik dní. Taková úloha byla 
označena za přesahující rozsah této práce. 

\subsection{Využití minibatch-learning}
Balík \ac{hpelm} nabízí možnost minibatch-learning, tj. rozdělení počátečního datasetu na několik menších, přičemž trénink probíhá 
iterativně. Model se natrénuje na prvním kusu dat a následně přidává vždy po jednom kusu a zpřesňuje model. Technika je 
dobrá jednak v případě, kdy se data nevejdou do paměti, jednak v reálné aplikaci, kdy se za provozu generují další data, kterými 
je možné model zpřesňovat.

Při tvorbě modelu byly používány kusy po \textasciitilde 50000 dvojicích. V průběhu tréninku (typicky 4krát) byl model 
otestován -- na trénovacích i testovacích datech. Bylo pozorováno postupné zhoršování přesnosti na tréninkových a zlepšování na 
testovacích (neznámých) datech. Všechny grafy z testování, společně s dalšími hodnotami, se ukládaly do generovaných 
protokolů -- příklad takového je v příloze~\ref{app:protocol}.

% \section{Aproximace neutronického kódu}
% 
% \subsection{Velká dimenzionalita}
% Při použití vstupů dle \eqref{eq:Pvec} se model masivně komplikuje. Při využití všech dostupných veličin je dimenze vstupních dat $28\cdot 121 = 3388$. 
% Obecně to zvyšuje nároky na výpočetní výkon i paměť.
% 
% K redukci dimenze dat při minimální ztrátě informace se běžně používá transformace Principal Component Analysis (PCA). Je-li na počátku soubor dat 
% o P reálných veličinách, je možné jej uvažovat jako množinu bodů v P-dimenzionálním prostoru. V něm je snaha najít vhodnější bázi. 
% V prvním kroku je počátek posunut do středu dat (ve smyslu průměrné hodnoty všech veličin). Následně se body proloží přímka $p$
% procházející počátkem (k její volbě se minimalizuje výraz 
% \eq{
% 	\sum_{i=1}^n d(\bv{x}_i, p)\,,
% 	\label{eq:pca_volba}
% }
% kde se $d$ volí jako eukleidovská vzdálenost. 
% Její směrový vektor je první vektor nové báze. Všechny zbývající bazické vektory se volí postupně tak, aby tvořily ortonormální soubor 
% vektorů a jejich přímky minimalizovaly výraz \eqref{eq:pca_volba}. 
% %Tak zvolená báze odpovídá hlavním osám P-dimenzionálního elipsoidu 
% Důležitá vlastnost takto zvolené báze je, že celkový rozptyl v jednotivých souřadnicích postupně klesá (tj. rozptyl okolo první osy je větší, než 
% okolo 2. atd.). Ortogonalita zajišťuje, že souřadnice nejsou příliš velké. PCA je tak dobré k nalezení shluků dat podobných vlastností a k redukci 
% dimenze. 
% Při výběru prvních $k<p$ souřadnic z PCA dochází k redukci dimenze při minimální ztrátě informace\footnotei{.}{Přesné nožství se dá vypočítat 
% a následně stanovit, kolik souřadnic je možné vynechat, aby ztráta informací nebyla příliš velká.} Snížení dimenzionality má pozitivní 
% efekt na rychlost tréninku, predikce a na paměťovou náročnost. 
% \dpic{bc_pca_60.png}{bc_60.png}{1200D model -- veličina bc.}{Model z 1200-dimenzionálních vstupů -- veličina bc (standardizováno).}{3388D model -- veličina bc.}{Model z 3388-dimenzionálních dat -- veličina bc (standardizováno).}
% 
% Ukázalo se, že dimenzi dat je možno výrazně snížit. Při pokusech byla provedena transformace PCA na vstupních datech. Z transformovaných dat bylo postupně 
% ponecháno 
% prvních 1200, 800, 600, 300, 80 a 28 souřadnic a ty byly použity pro trénink modelu. Porovnání modelu z původních a z vstupů transformovaných na dimenzi 
% 1200 je na obr. \ref{bc_pca_60.png}--\ref{bc_60.png} a \ref{mtc_pca_60.png}--\ref{mtc_60.png}. Ztráta přesnosti v modelu byla až na 200 vstupů minimální, rychlost predikce se zlepšovala.  
% %% sem obrázek
% %% \pic či \dpic ... před PCA, po PCA
% K použití v LPopt má PCA některé nedostatky. Krom lehce větší časové náročnosti (způsobeno nutností provést transformaci vstupních dat) jsou poblémy dva.
% 
% Prvním je, že transformace je získaná na základě počátečního datasetu. Vzhledem k velikosti konfiguračního prostoru však počáteční dataset nemusí 
% být reprezentativní vzorek. Transformace nalezená na počátku tedy nemusí nalézt obecně vhodné souřadnice -- pro jinou část konfiguračního prostoru 
% mohu být nevhodné. Získat reprezentativní vzorek ani nemusí být možné. 
% \dpic{mtc_pca_60.png}{mtc_60.png}{1200D model -- veličina mtc.}{Model z 1200-dimenzionálních vstupů - veličina mtc (standardizováno).}{3388D model -- veličina mtc.}{Model z 3388-dimenzionálních dat -- veličina mtc (standardizováno).}
% 
% Druhý problém souvisí s faktem, že PCA se tvoří pouze na základě vstupu, ale vůbec nezohledňuje význam veličin pro užitkovou funkci (výstup z neutronického 
% kódu). 
% Při redukci dimenze tak nestačí pouze vybrat prvních $k<NP$ veličin, ale také otestovat, zda model na nich natrénovaný poskytuje relevantní výstupy. 
% S ohledem na to je PCA možné použít jako \uv{počáteční odhad} -- podle něj vybrat veličiny, otestovat, zda je možné na nich natrénovat model, a následně 
% vybrat jako vstupy veličiny hojně obsažené v PCA, které zároveň dávají fyzikální smysl. 
% 
% Vstupy pro tvorbu modelu jsou v obou případech značně předimenzovány -- to demonstruje model natrénovaný pouze na 1 vstupní veličině ($k_{\infty}$) pro každou 
% pozici, tj. 28-dim. vstup. Problém spočívá v tom, že předem není známo, které veličiny mají na predikce modelu největší vliv.
% \dpic{bc_kinf.png}{mtc_kinf.png}{28D model -- veličina bc.}{Korelační graf pro model vytvořený z 28-dimenzionálních vstupů -- veličina bc.}{28D model -- veličina mtc.}{Korelační graf pro model vytvořený z 28-dimenzionálních vstupů -- veličina mtc.}


% \subsection{Distribuce výstupních hodnot}
% Z provedených experimentů plyne, že kvalita ML-modelu silně závisí na distribuci vstupních dat, resp. že naučený model je \uv{kopíruje}. Distribuce 
% vstupních dat jsou na obr. \ref{bc_dist.png} - \ref{mtc_dist.png}. 
% \dpic{bc_dist.png}{model_bc.png}{Distribuce hodnot \ac{bc}.}{Distribuce hodnot \ac{bc} v dostupných datech.}{Graf z predikce \ac{bc}.}{Korelační graf z predikce \ac{bc} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
% 
% K vhodnému fitu se hodí mít data z rovnoměrného rozdělení -- výrazná maxima nejsou žádoucí -- způsobují, že výsledný model funguje dobře na vstupech 
% z oblasti maxima distribuce, data nacházející se mimo tuto oblast fituje velmi špatně. To je dobře zřetelné z chování modelu na 
% parametrech \ac{fdh}, \ac{fha} a \ac{rc1}. Oblast 
% peaku je nafitována správně (žlutá oblast na ose $y=x$). Horizontální resp. vertikální oblasti vycházející z peaku odpovídají stavu, kdy model vyhodnotil data 
% z oblasti mimo peak nesprávně jako peaková, resp. data z peaku jako nepeaková. Tentýž jev pro více peaků jde vidět na obr.~\ref{model_pbu.png}.
% \dpic{fdh_dist.png}{model_fdh.png}{Distribuce hodnot \ac{fdh}.}{Distribuce hodnot \ac{fdh} v dostupných datech.}{Graf z predikce \ac{fdh}.}{Korelační graf z predikce \ac{fdh} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
% \dpic{fha_dist.png}{model_fha.png}{Distribuce hodnot \ac{fha}.}{Distribuce hodnot \ac{fha} v dostupných datech.}{Graf z predikce \ac{fha}.}{Korelační graf z predikce \ac{fha} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
% Tento jev je očekávaný a vyplývá z podstaty metody -- minimalizuje se průměrná chyba, nikoli její maximální hodnota. Nadbytek bodů v určité oblasti vede 
% k tomu, že model se zpřesňuje pouze v té oblasti, nikoli rovnoměrně na celém definičním oboru. 
% \dpic{rc1_dist.png}{model_rc1.png}{Distribuce hodnot \ac{rc1}.}{Distribuce hodnot \ac{rc1} v dostupných datech.}{Graf z predikce \ac{rc1}.}{Korelační graf z predikce \ac{rc1} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
% \dpic{pbu_dist.png}{model_pbu.png}{Distribuce hodnot \ac{pbu}.}{Distribuce hodnot \ac{pbu} v dostupných datech.}{Graf z predikce \ac{pbu}.}{Korelační graf z predikce \ac{pbu} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
% 
% Pro lepší představu je jev demonstrován na zašuměných datech 
% z polynomu 
% 4. stupně. K 100 bodům pocházejícím z rovnoměrného rozdělení z intervalu <0,1> a určité polynomiální funkce přidáme 150 jiných, vygenerovaných z podintervalu 
% <0.4,0.6> s jinou funkcí. Řádově stejný počet bodů výslednou funkci neovnivní a nafitovaná funkce bude vystihovat data s dobrou přesností na celém definičním 
% oboru~\ref{poly4_half_uni.png}. Bude-li počet bodů z podintervalu s jinou funkcí řádově vyšší, proloží nafitovaná funkce pouze onen podinterval (obr.~\ref{poly4_non_uni.png}). 
% \dpic{mtc_dist.png}{model_mtc.png}{Distribuce hodnot \ac{mtc}.}{Distribuce hodnot \ac{mtc} v dostupných datech.}{Graf z predikce \ac{mtc}.}{Korelační graf z predikce \ac{mtc} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
% \dpic{half_uni_dist.png}{poly4_half_uni.png}{Demonstrační funkce -- distribuce hodnot.}{Distribuce funkčních hodnot demonstrační funkce.}{Fit demonstrační funkce.}{Fit demonstrační funkce. Distribuce není příliš vychýlená, fit vystihuje date poměrně dobře.}
% \dpic{non_uni_dist.png}{poly4_non_uni.png}{Demonstrace -- distribuce nevhodně zvolených dat.}{Distribuce nevhodně zvolených dat -- funkčních hodnot je řádově víc v intervalu <0.4,0.6>.}{Demonstrace -- fit nevhodně zvolenými daty.}{Fit funkce danými daty. Výsledná funkce dobře popisuje pouze interval <0.4,0.6>.}
% 
% Trénovací a testovací data byla tvořena náhodným rozdělením 
% 
% % \subsection{Volba trénovacích dat}
% \uv{Patologické} distribuce odezev jsou důsledkem správného běhu optimalizačního algoritmu -- ten dle počátečního nastavení generoval vsázky tak, aby jejich 
% odezvy konvergovaly k zadaným hodnotám. Ostré peaky v distribuci pochází z jemné optimalizace v pozdější fázi běhu LPoptu, kdy se generovalo velké množství 
% vsázek s podobnými vlastnostmi. \uv{Pozadí} peaků pochází z počáteční fáze optimalizace, kdy se generovaly silně diverzní vsázky a optimalizační algoritmus 
% připomínal náhodné prohledávání. Jev je dobře viditelný např. v distribuci \ac{fdh}, \ac{fha}, \ac{rc1} na obr. \ref{fdh_dist.png}, \ref{fha_dist.png}, \ref{rc1_dist.png}.
% 
