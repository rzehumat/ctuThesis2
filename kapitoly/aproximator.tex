\chapter{Aproximace NFsolveru}
Za účelem urychlení procesu návrhu je možné nahradit nf-solver (Andrea 2D) 
zjednodušeným aproximátorem, který předpoví vybrané vlastnosti vsázky 
s dostatečnou přesností. 

Použití ML-metod negarantuje přesnost (ve smyslu omezení maximální 
chyby) -- to však nevadí, protože nejlepší návrhy z procesu optimalizace 
budou vždy nakonec ověřeny nf-solverem s maximální přesností.

% praskni tu obsah ml.tex
% pokusy s MATLAB toolboxem -- shallow nn, deep nn
% MATLAB -- regTreeEnsamble, baggedTrees
% pokusy s HPELM
% pokusy s TF

% Vzdy co jsme zkusili, obrazek, proc to nefungovalo, jak to vylepsit
% Aproximace NFsolveru
% =========================
% 
% 1) Vstupy pro pozice
% -- vstup 28 -- nechceme, zanedbáváme různé škály parametrů, a jejich různý význam
% 
% 2) Vstupy pro parametry
% -- vstup 121 -- nechceme, zanedbáváme různou významnost jednotlivých Pozic
% 
% 
% => reseni pro 1,2 - vazeni na veliciny, resp. pozice
% 
% --> problem - jak zvolit vazby -> nechame to, at si to model sám najde
% => vstup bude 28*121 = 3388 hodnot
% 
% = problém distribuce hodnot - potřeba upravit -> klasifikátor a N modelů? 
% (bo segmenty v korelačním diagramu)
% 
% Zmínit - pokusy s TF, různé normalizace, PCA,...
% -> reseni pomoci 2-krok. modelu?

\section{Požadavky na metodu}
Od modelu budeme požadovat, aby byl řádově rychlejší, než zjednodušený nf-solver (Andrea 2D) -- jinak by aproximace 
neměla význam. Očekávané zrychlení je $10^2 - 10^3$. 

Požadujeme možnost tzv. \ti{incremental learning}, tj. snadnou možnost do modelu přidávat nová trénovací data 
a tím jej za provozu zpřesňovat\footnotei{.}{To vylučuje např. \ti{regression tree ensambles}.} Tato vlastnost 
jde zajistit iterativním zpřesňováním\footnote{Např. \ti{extreme learning machines}.} nebo uložením parametrů modelu, 
novým tréninkem s přidáním dat, kde se uložené parametry použijí jako počáteční hodnoty.

%% incremental learning
%% rychla predikce 
%% zvladat velke dimenze -> potrebujeme vubec? Viz. pokus s PCA


\section{Tvorba dat}
Aby bylo možné model efektivně trénovat, musí data splňovat několik vlastností. V krocích popíšeme 
tvorbu takového modelu.
\begin{enumerate}
	\item Aby byl model dostatečně obecný, je žádoucí mít v trénovací množině uniformně rozložená data z každé oblasti 
		prostoru $\Omega_P$. Proto je nutné generovat data náhodným procházením prostoru, nikoli je brát z běhu 
		optimalizačního algoritmu (který ze své podstaty konverguje do malé oblasti). K tomu je dobré vybrat data 
		s velkou množinou různých palivových kazet -- například z několika různých kampaní.


	\item Palivové kazety popíšeme vhodnými veličinami. Ty musí být nezávislé a zcela popisovat palivovou kazetu. 
		Zároveň odstraníme veličiny, které jsou pro všechny kazety v trénovací množině konstantní -- zjevně totiž 
		neovlivní chování modelu. 
	
	\item Vytvoříme $\Omega_P^{tr}$ trénovací množinu. Ta je reprezentována maticí, kde řádky odpovídají jednotlivým 
		vsázkám, tj. $\Omega_P^{tr} [i,\cdot] = [\bv{P}_i, \bv{r}_i]$. Sloupce odpovídají jednotlivým veličinám, 
		popisujícím palivové kazety na různých pozicích, $\Omega_P^{tr} [\cdot, j] = [\bv{y}_{pos, vel}]$. 
		%% nejak smysluplne to pojmenuj!!!

	\item Data po sloupcích normalizujeme, tj. provedeme lineární transformaci 
		\begin{equation}
			\Omega_P^{tr,\ast} [\cdot,j] = \frac{y - \overline{y}}{y_{max} - y_{min}} \,.
		\end{equation}
		Tím bijektivně zobrazíme každou veličinu do intervalu <0,1>. To se hodí -- veličina zapsána relativně 
		vůči své řádové velikosti. Bez toho by model byl citlivý např. na lineární výkon, zatímco mikroskopické účinné 
		průřezy by považoval za 0. 

	\item Zkontrolujeme rozdělení v jednotlivých odezvách. Požadujeme rozdělení podobné uniformnímu, případně 
		normální rozdělení s nevýrazným maximem, bez příliš dlouhých \uv{chvostů} na okraji. Pokud rozdělení takové není, 
		provedeme výběr řádků tak, aby po výběru požadovanou distribuci mělo -- např. pomocí nižší pravděpodobnosti 
		výběru pro řádky s odezvou z oblasti nežádoucího peaku. 

	\item Provedeme PCA na vstupech\footnotei{.}{\ti{Principal Component Analysis} je lineární transformace, která převádí data tak, aby 
		střed byl v $\bv{0}$ a jednotlivé souřadnice odpovídaly hlavním osám elipsoidu, který data nejlépe popisuje. Z 
		podstaty metody plyne, že souřadnice v nové bázi budou lépe popisovat rozdělení hodnot ve fázovém prostoru -- tak, 
		že významnost souřadnic (ve smyslu poměrného množství rozptylu dat, které popisuje) bude klesající.} 
		Stanovme hladinu $k$ (např. $k = 99,5 \%$) a zanechme pouze prvních $l$ souřadnic tak, aby jejich významnost 
		byla $\geq k$. Tím výrazně zjednodušíme hledání modelu při minimální ztrátě 
		informace\footnotei{.}{Nutno si všimnout, že PCA nesplňuje požadavek na \ti{incremental learning}. Proto je potřeba 
		předpokládat, že rozložení hodnot počáteční množiny se přidáváním dalších dat nezmění.}

	\item Přetransformovaná data náhodně promícháme a ve vhodném poměru (např. 70:15:15) rozdělíme na trénovací, validační a 
		testovací množinu\footnotei{.}{Trénovací množina je soubor vstupů a odezev, na který se snažíme model nafitovat. 
		Náhodný výběr by měl zajišťovat jeho dostatečnou obecnost. Validační množina se používá k ověření kvality modelu 
		během tréninku. Pokud by model dobře fungoval na trénovacích datech, ale špatně na validačních či testovacích, 
		je potřeba zastavit se a zamyslet, kde se stala chyba.} 

	\item Na trénovacích datech vytvoříme vhodný model\footnotei{.}{Tím zpravidla rozumíme model, který data popisuje s 
		dostatečnou přesností a zároveň neobsahuje více parametrů, než je nutné.}

	\item Připraveným modelem provedeme predikci na testovacích vstupech, provedeme transformaci inverzní k normování 
		a porovnáme vzorové výstupy s výstupy modelu. Pokud je přesnost modelu uspokojivá, uložíme model, 
		normovací transformaci a PCA.

	\item Volitelně můžeme provádět další optimalizaci k zpřesnění či zrychlení modelu.



\end{enumerate}

% pokusy s 28 -> nezohlednuje vliv jednotlivych velicin
% PCA -- efekt: mene dat staci
% normalizace / standardizace

% \subsection{Distribuce hodnot}
% spike nechceme
% problem tezkeho overfittingu
% moznost udelat "prediktor-korektor" -- predictor tipne, do ktereho "spiku" to patří, poté se aplikuje nějaká lokální regrese

\section{Pokusy}
Při pokusech jsme používali soubor $10^6$ vsázek s vypočtenými odezvami $[BC, F_{\Delta H}, F_{HA}, RC1, PBU, MTC]$. Ukázalo se, že 
spolehlivý model není možné vytvořit kvůli špatné distribuci dat\footnotei{.}{Vzorová data byla vzata z optimalizačního procesu jedné temelínské kampaně.} 
Jelikož optimalizační algoritmus konverguje požadovaným hodnotám, jsou data výrazně vychýlená směrem k těmto hodnotám a nelze je pro tvorbu použít. 
\subsection{Regression trees}
Problém -- nejsou inkrementální, navíc obecně se pro tyto účely nepoužívají.
\subsection{ELM}
% nadefinovat metodu a balik hpelm
%% rychly trenink, pomala predikce -- bo shallow potřebuje hodně neuronů
Dobrá predikce, jen potřebuje fest neuronů


\subsection{Deep Neural Network}
Kvalita a distribuce predikcí značně odpovídá distribuci vstupních dat; problém overfittingu na peaky v distribuci vstupů je značný. 
Provedené výpočty naznačují, že lepší distribuce vstupů dokáže chybu výrazně zmenšit. 

Další byl velký vliv "nadbytečné dimenzionality" dat -- po vyloučení dat bez většího fyzikálního významu se dimenze vstupu 
snížila na \~ 2500 bez pozorovatelného zhoršení přesnosti, \~ 1200 po provedení PCA transformace. Některé veličiny dokonce 
byly zajímavě aproximovány i při omezení vstupu na 1 veličinu (bc při použití 27 kinf na vstupech). 

Potřebný počet neuronů ve vnitřních vrstvách je výrazně menší, než u jednovrstevné (srovnání stovky až tisíce neuronů 
v "shallow" a desítky až stovky v hluboké). Urychlení 10E+4 krát oproti nf kódu (Schlunz) zhruba odpovídalo 
experimentálnímu pozorování, že predikce odezev 1 vsázky zabrala řádově jednotky až desítku us.
% popsat - gradient descent, backpropagation, neurons, layers, overfitting/underfitting
% pomaly trenink
% problem spousty volnych parametru (LR, pocet vrstev, neuronu, typ neuronu, normalizace 
%%% inputs, normalizace outputs, regularizace (l2, l1, dropout, batchnorm), 
%%% aktivacni funkce (sigm, relu, tanh, swish, mish), optimizer (sgd, gd, Adam, rectAdam, 
%%% Ranger), earlyStop, reduceLROnPlateau, 
% popis TF API
% vyhoda hromadneho (radsi 200k nez 1) -- vektorizace, GPU, paralelizace,... 




%\chapter{Metody strojového učení}
% aproximace rychleho kodu necim rychlejsim -- dobre pouziti do heuristik
% motivace -- ta tvrzení (hornik, cybenko, clanek)
% univezalni zobrazovač
% aproximator vzdalenosti, "zobecnena vzdalenost"

% pokusy s MATLAB toolboxem -- shallow nn, deep nn
% MATLAB -- regTreeEnsamble, baggedTrees
% pokusy s HPELM
% pokusy s TF


% 
% -- co je ANN
% -- jak je muzeme vyuzit -- aproximace Andrey, aproximace vzdalenosti -> prohledavani okoli -> grad descent,...
% 
% 
% \chapter{Metody ANN}
% \section{Základy ANN}
% 
% % definici ANN s vice jak jednou hidden layer a volitelnym poctem neuronu v kazde vrstve? Pokud ano, poslete mi ukazku nejakeho matlab kodu, kde to definujete.
% % ANN s vice vrstvami
% 
% % https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%TOTO JE SPIS PRO ME, AT VIM%%%%%%%%%%%%%%%%%%
% \subsection{Dulezite vlastnosti}
 %Teorém \cite{cybenko} tvrdí, že ANN s 1 hidden layer s konečným počtem neuronů, užívající aktivačních funkcí typu sigmoid,  je schopna aproximovat libovolnou spojitou funkci. Přesnější formulace \cite{hornik} říká, že 
% 
% 
% %%POJMY
% \subsection{Zakladni pojmy}
% Artificial neuron je funkcionál složený s lineárním operátorem. Označme $(1, \vec{x})$  vektor (m+1tici čísel) z definičního oboru, tj. typicky získaný ze vstupních dat, a $(b, \vec{w})$ vektor. Výstupem neuronu pak je
% \begin{equation}
%     Out((1, \vec{x})) = \phi(b + \sum_{i=0}^{m} w_i x_i), 
% \end{equation}
% kde $b$ je bias a $\phi$ je nějaká funkce. Váhy jsou v průběhu učení modifikovány (viz dále).
% 
% Activation function $\phi$ rozhoduje, zda neuron má být aktivován či ne. Jedná se mj. o
% \begin{enumerate}
%     \item Threshold function $f(x) = 1$ pro $x \geq 0$, 0 jinak.
%     \item Sigmoid (logistická funkce) $f(x) = \frac{1}{1+\exp(-x)}$.
%     \item Hyperbolický tangens $\tanh(x)$.
%     \item Rectified linear units
% \end{enumerate}
% 
% Podle vstupů můžeme neurony skupit do vrstev (layers). Speciálně definujeme input layer jako neurony, jejichž vstupy jsou přímo vstupní data problému, a výstupní vrstvu (output layer) jako neurony, jejichž výstupem jsou přímo hodnoty, které by se v případě "dobře naučené" sítě nelišila od výstupu cvičných dat. 
% 
% Hidden layer je jakákoli vrstva neuronů, které není vstupní či výstupní. Správná volba počtu hidden layers a počtu neuronů v nich je velmi důležitá pro řešení problému, typicky s rostoucím počtem hidden layers se zvětšuje prostor funkcí, které ANN zvládne aproximovat. Podle počtu hidden layers můžeme rozlišit
% \begin{enumerate}
%   \item 0  schopna aproximovat po částech lineání funkce
%   \item 1 schopna aproximovat libovolnou spojitou funkci ?? (Na VP)
%   \item 2 
%   \item >2
% 
% \end{enumerate}
% Počet neuronů v jednotlivých vrstvách by měl být mezi počtem vstupů a výstupů a zároveň méně než dvojnásobek vstupů a okolo 2/3 vstupů + výsptupů.
% \section{Odhad transformace vzdáleností}
% Uvažujme, že máme aktivní zónu o $N$ pozicích. Poziční konfiguraci AZ (bez rotací) reprezentujme typovým vektorem $\vec{t}$ délky $N$, jehož prvky jsou čísla z $\{1,2,...,m'\}$, kde $m'$ je počet různých paliových souborů v sortimentu paliva. Pak $i$-tá složka vektoru $\vec{t}$ nám říká, jaký typ souboru se nachází na $i$-té pozici v AZ.
% Procesem mappingu (zobrazení, které každému typovému vektoru přiřadí neutornově-fyzikální charakteristiky jednotlivých palivových souborů dle katalogu paliva) převedeme $\vec{t}$ na matici (viz
% \ref{kap:vlastnosti-omega-LP} ZMIN TAM, ZE OMEGALP BIJ NA MATICE)
% \begin{equation}
%     \mathbb{P} = \left( \vec{p^{1}}, \vec{p^2}, ... , \vec{p^N}  \right) \,,
% \end{equation}
% kde $\vec{p^i}$ je sloupcový vektor neutronově-fyzikálních charakteristik palivového souboru na pozici $(\vec{t})_i$. Obecně nevíme, které n-f charkteristiky zahrnout do výpočtu. Z dosavadních zkušeností \ref{nekdo} se domníváme, že mezi relevantní n-f char. mohou patřit $k_{\infty}, BU, c(Gd), \sigma_F$. 
% 
% 
% 
% Pomocí neutronově-fyzikálního kódu vypočteme vektor odezev $\vec{r}_{res}$ (jeho prvky jsou např. celkový teplotní koeficient reaktivity, moderátorový koeficient reaktivity, délka kampaně, průměrné vyhoření na konci kampaně,...) ze znalosti $\mathbb{P}$.
% 
% %%%%%%%%%%%%%%%CHCI SPIS DO VZDALENOSTI NA OMEGA RES%%%%%%%%%%%%%%%%%%%
% 
% Pro jednoduchost zaveďme na $\Omega_{res}$ metriku indukovanou standardní Eukleidovskou normou $||.||_2$
% \begin{equation}
%       d(\vec{r}_{res,A},\vec{r}_{res,B}) = ||\vec{r}_{res,A} - \vec{r}_{res,B}||_2 = \sqrt{\sum_{k=1}^s ((\vec{r}_{res,A})_k - (\vec{r}_{res,A)_k}^2)},
% \end{equation}
% kde $s$ je počet relevantních n-f charakteristik palivové vsázky.
% 
% %%%%%%%%%%%%%%%CHCI SPIS DO VZDALENOSTI NA OMEGA RES%%%%%%%%%%%%%%%%%%%
% 
% 
% 
% Můžeme využít funkci \verb|pdist2(A,B,|. 
% 
% 
% Na $\Omega_{LP}$ zavedeme posloupcovou metriku 
% \begin{equation}
%   ||\Delta \vec{p}_{A,B}^{î}|| = \frac{||\vec{p}_{A}^{i} - \vec{p}_{B}^{i}||_{2}}{k} \,,
% \end{equation}
% kde $i$ je sloupcový index matice $\mathbb{P}_A$, tedy $\vec{p}_A^i = \mathbb{P}_A[\cdot, i]$ je $i$-tý sloupec matic $\mathbb{P}_A$. Číslo $k$ musíme volit tak, aby 
% $||\Delta \vec{p}_{A,B}^{î}||$ splňovalo, že
% \begin{itemize}
%       \item $\forall A,B: ||\Delta \vec{p}_{A,B}^{î}|| \leq 1$,
%       \item $\forall A,B,C,D: ||\vec{p}_A^i - \vec{p}_B^i||_2 \geq ||\vec{p}_C^i - \vec{p}_D^i||_2 \rightarrow ||\Delta \vec{p}_{A,B}^î|| \geq ||\Delta \vec{p}_{C,D}^î||$
%       %%\item $\forall A,B,C,D \neq \mathbb{O}: ||\Delta \vec{p}_{A,B}^î||=||\Delta \vec{p}_{C,D}^î|| \Leftrightarrow A=C \wedge B=D$.
% \end{itemize}
% Ukážeme, že volba $k=2max_{i \in N}||\Delta \vec{p}_{A,B}^î||_2$ tyto podmínky splňuje. První vlastnost je splněna triviálně z trojúhelníkové nerovnosti v definici normy. Bodu 2 by odpovídalo vydělení konstantou (vzhledem k volbě sloupce). Tomu naše volba odpovídá.
% 
% Neuronové síti dáme velké množství dat $\Delta \vec{p}_{A,B}^1, \Delta \vec{p}_{A,B}^,...,\Delta \vec{p}_{A,B}^N)$ a jim odpovídající $ d(\vec{r}_{res,A},\vec{r}_{res,B})$, která získáme napočítáním všech dvojic z dat z velkého množství dříve napočtených vsázek. Neuronová síť si pak vytvoří model z $N$ funkcionálů $\phi^1, \phi^2,...,\phi^N$ takových, že ze vstupu $\Delta \vec{p}_{A,B}^1, \Delta \vec{p}_{A,B}^,...,\Delta \vec{p}_{A,B}^N)$ vypočtou $d(\vec{r}_{res,A},\vec{r}_{res,B})$. Využíváme velké množství dat, protože z povahy neuronových sítí plyne, že s každý dalším trénovacím prvkem je síť přesnější. 
% 
% Po dostatečném natrénování sítě provedeme její validaci. Ta bude spočívat v tom, že jako vstup budeme dávat vzdálenosti vsázek, které nebyly v tréninkové množině, a budeme hodnotit, jak se odhad neutronové sítě liší od skutečného $d(\vec{r}_{res,A},\vec{r}_{res,B})$ vypočteného neutronickým kódem. Jesliže pak průměrná odchylka odhadu $\overline{\Delta \varphi} := d*(\vec{r}_{res,A},\vec{r}_{res,B}) - d(\vec{r}_{res,A},\vec{r}_{res,B})$, kde $d*(\vec{r}_{res,A},\vec{r}_{res,B})$ je odhad neuronové sítě, bude menší než empiricky stanovená prahov hodnota úspěšnosti $\overline{\Delta \varphi}_{max}$, budeme mít dobrý odhad vzdálenosti v $\Omega_{res}$ na základě znalosti vzdálenosti z $\Omega_{LP}$. Z funkcionálů $\phi^1, \phi^2,...,\phi^N$ pak můžeme určit souvislost mezi vzdálenostmi na těch 2 prostorech.
% 
% %Pak pomocí ANN budeme hledat $N$tici funkcionálů $\phi^1, \phi^2,...,\phi^N$, kde $\forall i \in N: \sum_{i=1}^N \phi^i(\Delta \vec{p}_{A,B}^i) = d(\vec{r}_{res,A},\vec{r}_{res,B})$. 
% %Za účelem hledání těchto funkcionálů vezmeme data o velkém množství vsázek\footnote{Využijeme vlastnosti neuronových sítí, že jejich } a pro všechny dvojice $(\mathbb{A},\mathbb{B}$ z $\Omega_{LP}$ napočítáme $\Delta \vec{p}_{A,B}^1, \Delta \vec{p}_{A,B}^,...,\Delta \vec{p}_{A,B}^N)$ a jim odpovídající $ d(\vec{r}_{res,A},\vec{r}_{res,B})$. 
% 
% %Takto připravená data pak předáme neuronové síti. Najde-li dostatečně přesně $N$tici funkcionálů, která dobře aproximuje funkci $f: d(\mathbb{A},\mathbb{B})\rightarrow d(\vec{r}_{res,A},\vec{r}_{res,B})$. Tedy bychom dostali velmi přesnou informaci, jak souvisí vzdálenosti na $\Omega_{LP}$ se vzdálenostmi v $\Omega_{res}$.
% Nutno tam zmínit, že provádět jen tak bezmyšlenkovitě $\Delta p_{A,B}^i$ je fyzikálně špatně, neboť soubory na jednotlivých pozicích nejsou nezávislé, nýbrž jsou ovlivněny svým okolím (ve smyslu souborů na okolních pozicích v AZ) - pro příklad uveďme situaci, kdy má jistý soubor v AZ vedle sebe čerstvý soubor, generující velké množství neutronů, což obecně bude zvyšovat množství probíhajících reakcí (ve srovnání se situací, kdy by vedle byl soubor s velkým vyhořením). Pro začátek předpokládejme, že to neuronová síť zvládne a vrátí vyhovující funkcionály.
% 
% Všude, kde jsme užívali $||.||_2$, byla tato volba neodůvodněná a obecně špatně. Šlo pouze o naše zjednodušení, abychom zjednodušili výpočty. Pro korektnost výpočtu bychom tam měli hledat obecné metriky, které budou zlepšovat výsledky. Obecné metriky by však mohly být nevhodné k reprezentaci v počítači. Proto se omezme na třídu vážených $p$-metrik definovaných jako
% \begin{equation}
%       d(\vec{a}, \vec{b}) = \sqrt[p]{\vec{(a-b)^{.p}}^T \mathbb{W} \vec{(a-b)^{.p}} },
% \end{equation}
% kde $p>1$, $\mathbb{W}$ je pozitivně definitní symetrická matice, vektory uvažujeme sloupcové a $a^{.p}$ značí vektor umocněný po složkách (element-wise), tedy vznikl umocněním všech složek původního vektoru\footnote{úlohu jsme takto původně neřešili, protože nám vzrůstá počet vnořených cyklů a náročnost úlohy rychle roste.}.
% 
% \section{Metody ANN obecně}
% Metody ANN jsou založeny na postupném zpřesňování modelu mezi vstupy a výstupy. Jsou navrženy, aby "fitnuly téměř cokoliv". Probíhají ve 3 krocích:
% \begin{enumerate}
%     \item učení - v této fázi algoritmu předkládáme známé dvojice vstup-výstup (např. vzor-obraz při neznámém zobrazení, prvek z množiny (a jeho vlastnosti) - a jeho klasifikace,...). Čím větší množství dat k učení ANN dáme, tím lépe by měla dané zobrazené najít.
%     \item validace
%     \item testování - model získaný učením testujeme na vstupy (jejichž výstupy neznáme). Poté porovnáme výsledky neutronové síte a skutečná výstupní data. 
% \end{enumerate}
% 1) u
% 
% \section{Metody ANN - implementace v MATLABu}
% Implementace metod ANN v MATLABu zahrnuje metody pro aproximace funkcí (to chceme), rozpoznání obrazu, predikce vývoje a další.
% 
% Volíme aproximaci funkcí. Vstupem je 1 matice $R^{mn}$, kde $n$ je počet vstupních dat a $m$ počet parametrů příslušejících každému vstupu. V našem případě by $n$ odpovídalo počtu dvojic vsázek, $m$ počtu $\Delta p_{A,B}^i$, tj. počtu pozic v AZ. Výstupní matice bude typu $R^{1,n}$, tj. řádkovému vektoru, jehož $j$-tý prvek odpovídá vzdálenosti odezev 2 vsázek. 
% 
% %https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf
% V MATLABu je nejjednodušší metodou metoda Shallow Neural Network.
% 
% 
% 

