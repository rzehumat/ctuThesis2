\chapter{Aproximace neutronického kódu}
K ověření možnosti využití \ac{ml}-metod k úlohám \ac{micfmo} bylo rozhodnuto vyzkoušet aproximovat neutronický kódu Andrea 2D 
pomocí \ac{hpelm} nebo jiné implementace neuronových sítí. 

Podobná úloha už byla několikrát řešena a implementována.
\cit{
\item Schlünz~\cite{schlunz} implementoval \ac{ann} k predikci neutronového toku, výkonu, nevyrovnání výkonu a váhy kontrolních 
	a havarijních elementů reaktoru SAFARI-1. Vstup tvořily hmotnosti \ce{^{235}U} na jednotlivých pozicích v \ac{az}, 
		výstup vždy pouze jedna veličina. Dosažena chyba v průměru $< 2 \%$, maximální $<11\%$. Vstupní data byla generována 
		jako náhodné permutace. Vytvořený model byl přesný pouze pro predikci v rámci kampaně, na které byl natrénován.
	\item Jiang et al~\cite{jiang} implementoval \ac{ann} k predikci $k_{eff}$ (celé \ac{az}) z $k_{eff}$ souborů na jednotlivých 
		pozicích v \ac{az} ve výzkumném reaktoru CONSORT\footnotei{.}{Bývalý výzkumný reaktor na Imperial College London. Vyřazen z provozu 
		v roce 2012 z důvodu vzrůstajících nákladů na provoz a nedostatečnému využívání \cite{wnn}.} Dosažena maximální chyba $<2 \%$.
	\item Erdognan a Geckingli~\cite{erdogan} implementovali \ac{ann} k predikci $k_{eff}$ s přesností $<1\%$ (na datech z provozu \ac{je} 
		Almaraz\footnote{\ac{je} ve Španělsku. Zahrnuje 2 bloky PWR o celkovém výkonu 2017 \jdt{MW_e.}}).
}
Zrychlení oproti nodálním kódům je až $10^4$.  

Aproximace kódu Andrea 2D může je poněkud složitější úlohou z důvodu
\cit{
\item většího počtu pozic v \ac{az} (1/6 \ac{vver1000}),
	\item větší rozmanitost palivových souborů (použitá data obsahovala 25 souborů v 6 rotacích, různých souborů může 
		být v principu nekonečně mnoho vlivem různých pozic v \ac{az}, různých verzích paliva a odlišnosti jednotivých 
		kampaní,
	\item přítomnost vyhořívajících absorbátorů,
	\item větší počet parametrů paliva (apriorně není známo, které jsou důležité).
}

\section{Použité metody}
S ohledem na předchozí zkušenost s tréninkem na větším množství dat byly použity metody \ac{hpelm} a \ac{dnn}. Používal se celý dataset rozdělený 
zpravidla v poměru 80 \% trénovacích a 20 \% testovacích dvojic, případně 70 \% trénovacích, 15 \% validačních a 15 \% testovacích dvojic. 
Validační data byla použita v průběhu \ti{back-propagation} tréninku \ac{dnn} k zkvalitnění procesu díky průběžného testování modelu 
(tzv.~\ti{validace}).

\ac{dnn} je druh \ac{ann} využívající více hustě propojených\footnote{Hustě propojená hluboká síť (\ti{dense network}) znamená, že všechny neurony každé 
skryté vrsvy mají za vstup výstupy všech neuronů předchozí vrstvy.} skrytých vrstev. Díky tomu dosahují výrazně vyšší komplexity exponenciálně 
rostoucí s počtem skrytých vrstev. To také snižuje potřebný počet neuronů. 
Nevýhodou je pomalejší proces 
tréninku. 

Metoda \ac{dnn} implementována v balíku TensorFlow verze 2.0.0 pro jazyk Python \cite{tensorflow}. TensorFlow poskytovalo speciální metody 
a datové typy optimalizované pro 
trénink \ac{dnn} na \ac{gpu} a taktéž vyčerpávající možnosti nastavení vnitřních parametrů modelu, typů aktivačních funkcí. Další praktická výhodou je 
rozsáhlý back-end pro ladění vytvořených modelů a jejich implementaci modelu do provozu. 

Napsání funkčního kódu v TensorFlow bylo oproti \ac{hpelm} složitější kvůli rozsáhlosti balíku. Poněkud nejasná zůstala otázka inkrementálního 
tréniku. Pro velká data přesahující paměťové možnosti počítače byla dostupná metoda \verb|tf.data.Dataset| a parametr \verb|batchsize|, možnost 
přidávání trénovacích dat do existujícího modelu však v rámci této práce nebyla ověřována.

Oběma metodami bylo dosaženo podobných výsledků, které jen potvrdily předchozí domněnky o nevhodně zvolené množině trénovacích dat. 
\ac{dnn} s řádově desítkami neuronů (celkem ve všech skrytých vrstvách) dosahovaly stejné přesnosti jako \acs{hpelm} s řádově tisíci 
neurony. To vedlo k rychlejší predikci -- predikce jedné vsázky byla $\approx 20 \upmu$ s u \ac{dnn} oproti $\approx 40 \upmu$ s u \ac{hpelm}.
V důsledku \ti{back-propagation} byl trénink \ac{dnn} delší než \ac{hpelm} -- řádově desítky minut na \ac{gpu} oproti jednotkám minut u \ac{hpelm} 
při stejné hardwarové konfiguraci.
 
\section{Nalezené problémy}
Vytvořené modely nedosahovaly požadované přesnosti predikce, což jen potvrdilo domněnky o složitosti úlohy a nevhodně zvolených datech. 
Při tréninku byly odhaleny další problémy.

\subsection{Velká dimenzionalita}
Při použití vstupů dle \eqref{eq:Pvec} se model masivně komplikuje. Při využití všech dostupných veličin je dimenze vstupních dat $28\cdot 121 = 3388$. 
Obecně to zvyšuje nároky na výpočetní výkon i paměť.

K redukci dimenze dat při minimální ztrátě informace se běžně používá transformace Principal Component Analysis (\ac{pca}). Je-li na počátku soubor dat 
o $P$ reálných veličinách, je možné jej uvažovat jako množinu bodů v $P$-dimenzionálním prostoru. V něm je snaha najít vhodnější bázi. 
V prvním kroku je počátek posunut do středu dat (ve smyslu průměrné hodnoty všech veličin). Následně se body proloží přímka $p$
procházející počátkem -- k její volbě se minimalizuje výraz 
\eq{
	\sum_{i=1}^n d(\bv{x}_i, p)\,,
	\label{eq:pca_volba}
}
kde se $d$ volí jako eukleidovská vzdálenost. 
Její směrový vektor je první vektor nové báze. Všechny zbývající bazické vektory se volí postupně tak, aby tvořily ortonormální soubor 
vektorů a jejich přímky minimalizovaly výraz \eqref{eq:pca_volba}. 
%Tak zvolená báze odpovídá hlavním osám P-dimenzionálního elipsoidu 
Důležitá vlastnost takto zvolené báze je, že celkový rozptyl v jednotivých souřadnicích postupně klesá (tj. rozptyl okolo první osy je větší, než 
okolo 2. atd.). Ortogonalita zajišťuje, že souřadnice nejsou příliš velké. \ac{pca} je tak dobré k nalezení shluků dat podobných vlastností a k redukci 
dimenze. 
Při výběru prvních $k<p$ souřadnic z \ac{pca} dochází k redukci dimenze při minimální ztrátě informace -- přesné množství se dá vypočítat 
a následně stanovit, kolik souřadnic je možné vynechat, aby ztráta informací nebyla větší, než stanovená maximální tolerance. Snížení dimenzionality má pozitivní 
efekt na rychlost tréninku, predikce a na paměťovou náročnost. 
\dpic{bc_pca_60.png}{bc_60.png}{1200D model -- veličina \acs{bc}.}{Model z 1200-dimenzionálních vstupů -- veličina \acs{bc} (standardizováno). Červená přímka je osa $y=x$, černé čáry značí toleranci 5 \%, tj. $y = (1\pm0,05)x$.}{3388D model -- veličina \acs{bc}.}{Model z 3388-dimenzionálních dat -- veličina \acs{bc} (standardizováno).}

Ukázalo se, že dimenzi dat je možno výrazně snížit. Při pokusech byla provedena transformace \ac{pca} na vstupních datech. Z transformovaných dat bylo postupně 
ponecháno 
prvních 1200, 800, 600, 300, 80 a 28 souřadnic a ty byly použity pro trénink modelu. Porovnání modelu z původních a z vstupů transformovaných na dimenzi 
1200 je na obr. \ref{bc_pca_60.png}--\ref{bc_60.png} a \ref{mtc_pca_60.png}--\ref{mtc_60.png}. Ztráta přesnosti v modelu byla až na 200 vstupů minimální, rychlost predikce se zlepšovala.  
%% sem obrázek
%% \pic či \dpic ... před \ac{pca}, po \ac{pca}
K použití v LPopt má \ac{pca} určité nedostatky. Krom lehce větší časové náročnosti, způsobené nutností provést lineární transformaci vstupních dat, jsou 
poblémy dva.

Prvním je, že transformace je získaná na základě počátečního datasetu. Vzhledem k velikosti konfiguračního prostoru však počáteční dataset nemusí 
být reprezentativní vzorek. Transformace nalezená na počátku tedy nemusí nalézt obecně vhodné souřadnice -- pro jinou část konfiguračního prostoru 
mohu být nevhodné. Získat reprezentativní vzorek ani nemusí být možné. 
\dpic{mtc_pca_60.png}{mtc_60.png}{1200D model -- veličina \acs{mtc}.}{Model z 1200-dimenzionálních vstupů - veličina \acs{mtc} (standardizováno). Červená přímka je osa $y=x$, černé čáry značí toleranci 5 \%, tj. $y = (1\pm0,05)x$.}{3388D model -- veličina \acs{mtc}.}{Model z 3388-dimenzionálních dat -- veličina \acs{mtc} (standardizováno).}

Druhý problém souvisí s faktem, že \ac{pca} se tvoří pouze na základě vstupu, ale vůbec nezohledňuje význam veličin pro užitkovou funkci (výstup z neutronického 
kódu). 
Při redukci dimenze tak nestačí pouze vybrat prvních $k<NP$ veličin, ale také otestovat, zda model na nich natrénovaný poskytuje relevantní výstupy. 
S ohledem na to je \ac{pca} možné použít jako \uv{počáteční odhad} -- podle něj vybrat veličiny, otestovat, zda je možné na nich natrénovat model, a následně 
vybrat jako vstupy veličiny hojně obsažené v \ac{pca}, které zároveň dávají fyzikální smysl. 

Vstupy pro tvorbu modelu jsou v obou případech značně předimenzovány -- to demonstruje model natrénovaný pouze na 1 vstupní veličině ($k_{\infty}$) pro každou 
pozici, tj. 28-dim. vstup. Problém spočívá v tom, že předem není známo, které veličiny mají na predikce modelu největší vliv.
\dpic{bc_kinf.png}{mtc_kinf.png}{28D model -- veličina \acs{bc}.}{Korelační graf pro model vytvořený z 28-dimenzionálních vstupů -- veličina \acs{bc}.}{28D model -- veličina \acs{mtc}.}{Korelační graf pro model vytvořený z 28-dimenzionálních vstupů -- veličina \acs{mtc}.}

\subsection{Distribuce výstupních hodnot}
Z provedených experimentů plyne, že kvalita \ac{ml}-modelu silně závisí na distribuci vstupních dat, resp. že naučený model je \uv{kopíruje}. Distribuce 
vstupních dat jsou na obr. \ref{bc_dist.png} - \ref{mtc_dist.png}. 
\dpic{bc_dist.png}{model_bc.png}{Distribuce hodnot \acs{bc}.}{Distribuce hodnot \acs{bc} v dostupných datech.}{Graf z predikce \acs{bc}.}{Korelační graf z predikce \acs{bc} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}

K vhodnému fitu se hodí mít data z rovnoměrného rozdělení -- výrazná maxima nejsou žádoucí -- způsobují, že výsledný model funguje dobře na vstupech 
z oblasti maxima distribuce, data nacházející se mimo tuto oblast fituje velmi špatně. To je dobře zřetelné z chování modelu na 
parametrech \ac{fdh}, \ac{fha} a \ac{rc1}. Oblast 
peaku je nafitována správně (žlutá oblast na ose $y=x$). Horizontální resp. vertikální oblasti vycházející z peaku odpovídají stavu, kdy model vyhodnotil data 
z oblasti mimo peak nesprávně jako peaková, resp. data z peaku jako nepeaková. Tentýž jev pro více peaků jde vidět na obr.~\ref{model_pbu.png}.
\dpic{fdh_dist.png}{model_fdh.png}{Distribuce hodnot \acs{fdh}.}{Distribuce hodnot \acs{fdh} v dostupných datech.}{Graf z predikce \acs{fdh}.}{Korelační graf z predikce \acs{fdh} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
\dpic{fha_dist.png}{model_fha.png}{Distribuce hodnot \acs{fha}.}{Distribuce hodnot \acs{fha} v dostupných datech.}{Graf z predikce \acs{fha}.}{Korelační graf z predikce \acs{fha} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
Tento jev je očekávaný a vyplývá z podstaty metody -- minimalizuje se průměrná chyba, nikoli její maximální hodnota. Nadbytek bodů v určité oblasti vede 
k tomu, že model se zpřesňuje pouze v té oblasti, nikoli rovnoměrně na celém definičním oboru. 
\dpic{rc1_dist.png}{model_rc1.png}{Distribuce hodnot \acs{rc1}.}{Distribuce hodnot \acs{rc1} v dostupných datech.}{Graf z predikce \acs{rc1}.}{Korelační graf z predikce \acs{rc1} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
\dpic{pbu_dist.png}{model_pbu.png}{Distribuce hodnot \acs{pbu}.}{Distribuce hodnot \acs{pbu} v dostupných datech.}{Graf z predikce \acs{pbu}.}{Korelační graf z predikce \acs{pbu} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}

Pro lepší představu je jev demonstrován na zašuměných datech 
z polynomu 
4.~stupně. K 100 bodům pocházejícím z rovnoměrného rozdělení z intervalu $\langle0,1\rangle$ a určité polynomiální funkce přidáme 150 jiných, vygenerovaných z podintervalu 
$\langle0,4, 0,6\rangle$ s jinou funkcí. Řádově stejný počet bodů výslednou funkci neovlivní a nafitovaná funkce bude vystihovat data s dobrou přesností na celém definičním 
oboru~\ref{poly4_half_uni.png}. Bude-li počet bodů z podintervalu s jinou funkcí řádově vyšší, proloží nafitovaná funkce pouze onen podinterval (obr.~\ref{poly4_non_uni.png}). 
\dpic{mtc_dist.png}{model_mtc.png}{Distribuce hodnot \acs{mtc}.}{Distribuce hodnot \acs{mtc} v dostupných datech.}{Graf z predikce \acs{mtc}.}{Korelační graf z predikce \acs{mtc} modelem. Na ose x jsou skutečné hodnoty, na ose y predikce.}
\dpic{half_uni_dist.png}{poly4_half_uni.png}{Demonstrační funkce -- distribuce hodnot.}{Distribuce funkčních hodnot demonstrační funkce.}{Fit demonstrační funkce.}{Fit demonstrační funkce. Distribuce není příliš vychýlená, fit vystihuje data poměrně dobře.}
\dpic{non_uni_dist.png}{poly4_non_uni.png}{Demonstrace -- distribuce nevhodně zvolených dat.}{Distribuce nevhodně zvolených dat -- funkčních hodnot je řádově víc v intervalu $\langle0,4; 0,6\rangle$.}{Demonstrace -- fit nevhodně zvolenými daty.}{Fit funkce danými daty. Výsledná funkce dobře popisuje pouze interval $\langle0,4; 0,6\rangle$.}

Trénovací a testovací data byla tvořena náhodným rozdělením. 

% \section{Volba trénovacích dat}
\uv{Patologické} distribuce odezev jsou důsledkem správného běhu optimalizačního algoritmu -- ten dle počátečního nastavení generoval vsázky tak, aby jejich 
odezvy konvergovaly k zadaným hodnotám. Ostré peaky v distribuci pochází z jemné optimalizace v pozdější fázi běhu LPoptu, kdy se generovalo velké množství 
vsázek s podobnými vlastnostmi. \uv{Pozadí} peaků pochází z počáteční fáze optimalizace, kdy se generovaly silně diverzní vsázky a optimalizační algoritmus 
připomínal náhodné prohledávání. Jev je dobře viditelný např. v distribuci \ac{fdh}, \ac{fha}, \ac{rc1} na obr. \ref{fdh_dist.png}, \ref{fha_dist.png}, \ref{rc1_dist.png}.

