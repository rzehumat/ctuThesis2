% TU NIC NENI, UZ JE TO ZKOPIROVANE DO APROXIMATORU

%\chapter{Metody strojového učení}
% aproximace rychleho kodu necim rychlejsim -- dobre pouziti do heuristik
% motivace -- ta tvrzení (hornik, cybenko, clanek)
% univezalni zobrazovač
% aproximator vzdalenosti, "zobecnena vzdalenost"

% pokusy s MATLAB toolboxem -- shallow nn, deep nn
% MATLAB -- regTreeEnsamble, baggedTrees
% pokusy s HPELM
% pokusy s TF


% 
% -- co je ANN
% -- jak je muzeme vyuzit -- aproximace Andrey, aproximace vzdalenosti -> prohledavani okoli -> grad descent,...
% 
% 
% \chapter{Metody ANN}
% \section{Základy ANN}
% 
% % definici ANN s vice jak jednou hidden layer a volitelnym poctem neuronu v kazde vrstve? Pokud ano, poslete mi ukazku nejakeho matlab kodu, kde to definujete.
% % ANN s vice vrstvami
% 
% % https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%TOTO JE SPIS PRO ME, AT VIM%%%%%%%%%%%%%%%%%%
% \subsection{Dulezite vlastnosti}
% Teorém \cite{cybenko} tvrdí, že ANN s 1 hidden layer s konečným počtem neuronů, užívající aktivačních funkcí typu sigmoid,  je schopna aproximovat libovolnou spojitou funkci. Přesnější formulace \cite{hornik} říká, že 
% 
% 
% %%POJMY
% \subsection{Zakladni pojmy}
% Artificial neuron je funkcionál složený s lineárním operátorem. Označme $(1, \vec{x})$  vektor (m+1tici čísel) z definičního oboru, tj. typicky získaný ze vstupních dat, a $(b, \vec{w})$ vektor. Výstupem neuronu pak je
% \begin{equation}
%     Out((1, \vec{x})) = \phi(b + \sum_{i=0}^{m} w_i x_i), 
% \end{equation}
% kde $b$ je bias a $\phi$ je nějaká funkce. Váhy jsou v průběhu učení modifikovány (viz dále).
% 
% Activation function $\phi$ rozhoduje, zda neuron má být aktivován či ne. Jedná se mj. o
% \begin{enumerate}
%     \item Threshold function $f(x) = 1$ pro $x \geq 0$, 0 jinak.
%     \item Sigmoid (logistická funkce) $f(x) = \frac{1}{1+\exp(-x)}$.
%     \item Hyperbolický tangens $\tanh(x)$.
%     \item Rectified linear units
% \end{enumerate}
% 
% Podle vstupů můžeme neurony skupit do vrstev (layers). Speciálně definujeme input layer jako neurony, jejichž vstupy jsou přímo vstupní data problému, a výstupní vrstvu (output layer) jako neurony, jejichž výstupem jsou přímo hodnoty, které by se v případě "dobře naučené" sítě nelišila od výstupu cvičných dat. 
% 
% Hidden layer je jakákoli vrstva neuronů, které není vstupní či výstupní. Správná volba počtu hidden layers a počtu neuronů v nich je velmi důležitá pro řešení problému, typicky s rostoucím počtem hidden layers se zvětšuje prostor funkcí, které ANN zvládne aproximovat. Podle počtu hidden layers můžeme rozlišit
% \begin{enumerate}
%   \item 0  schopna aproximovat po částech lineání funkce
%   \item 1 schopna aproximovat libovolnou spojitou funkci ?? (Na VP)
%   \item 2 
%   \item >2
% 
% \end{enumerate}
% Počet neuronů v jednotlivých vrstvách by měl být mezi počtem vstupů a výstupů a zároveň méně než dvojnásobek vstupů a okolo 2/3 vstupů + výsptupů.
% \section{Odhad transformace vzdáleností}
% Uvažujme, že máme aktivní zónu o $N$ pozicích. Poziční konfiguraci AZ (bez rotací) reprezentujme typovým vektorem $\vec{t}$ délky $N$, jehož prvky jsou čísla z $\{1,2,...,m'\}$, kde $m'$ je počet různých paliových souborů v sortimentu paliva. Pak $i$-tá složka vektoru $\vec{t}$ nám říká, jaký typ souboru se nachází na $i$-té pozici v AZ.
% 
% Procesem mappingu (zobrazení, které každému typovému vektoru přiřadí neutornově-fyzikální charakteristiky jednotlivých palivových souborů dle katalogu paliva) převedeme $\vec{t}$ na matici (viz
% \ref{kap:vlastnosti-omega-LP} ZMIN TAM, ZE OMEGALP BIJ NA MATICE)
% \begin{equation}
%     \mathbb{P} = \left( \vec{p^{1}}, \vec{p^2}, ... , \vec{p^N}  \right) \,,
% \end{equation}
% kde $\vec{p^i}$ je sloupcový vektor neutronově-fyzikálních charakteristik palivového souboru na pozici $(\vec{t})_i$. Obecně nevíme, které n-f charkteristiky zahrnout do výpočtu. Z dosavadních zkušeností \ref{nekdo} se domníváme, že mezi relevantní n-f char. mohou patřit $k_{\infty}, BU, c(Gd), \sigma_F$. 
% 
% 
% 
% Pomocí neutronově-fyzikálního kódu vypočteme vektor odezev $\vec{r}_{res}$ (jeho prvky jsou např. celkový teplotní koeficient reaktivity, moderátorový koeficient reaktivity, délka kampaně, průměrné vyhoření na konci kampaně,...) ze znalosti $\mathbb{P}$.
% 
% %%%%%%%%%%%%%%%CHCI SPIS DO VZDALENOSTI NA OMEGA RES%%%%%%%%%%%%%%%%%%%
% 
% Pro jednoduchost zaveďme na $\Omega_{res}$ metriku indukovanou standardní Eukleidovskou normou $||.||_2$
% \begin{equation}
% 	d(\vec{r}_{res,A},\vec{r}_{res,B}) = ||\vec{r}_{res,A} - \vec{r}_{res,B}||_2 = \sqrt{\sum_{k=1}^s ((\vec{r}_{res,A})_k - (\vec{r}_{res,A)_k}^2)},
% \end{equation}
% kde $s$ je počet relevantních n-f charakteristik palivové vsázky.
% 
% %%%%%%%%%%%%%%%CHCI SPIS DO VZDALENOSTI NA OMEGA RES%%%%%%%%%%%%%%%%%%%
% 
% 
% 
% Můžeme využít funkci \verb|pdist2(A,B,|. 
% 
% 
% Na $\Omega_{LP}$ zavedeme posloupcovou metriku 
% \begin{equation}
%   ||\Delta \vec{p}_{A,B}^{î}|| = \frac{||\vec{p}_{A}^{i} - \vec{p}_{B}^{i}||_{2}}{k} \,,
% \end{equation}
% kde $i$ je sloupcový index matice $\mathbb{P}_A$, tedy $\vec{p}_A^i = \mathbb{P}_A[\cdot, i]$ je $i$-tý sloupec matic $\mathbb{P}_A$. Číslo $k$ musíme volit tak, aby 
% $||\Delta \vec{p}_{A,B}^{î}||$ splňovalo, že
% \begin{itemize}
% 	\item $\forall A,B: ||\Delta \vec{p}_{A,B}^{î}|| \leq 1$,
% 	\item $\forall A,B,C,D: ||\vec{p}_A^i - \vec{p}_B^i||_2 \geq ||\vec{p}_C^i - \vec{p}_D^i||_2 \rightarrow ||\Delta \vec{p}_{A,B}^î|| \geq ||\Delta \vec{p}_{C,D}^î||$
% 	%%\item $\forall A,B,C,D \neq \mathbb{O}: ||\Delta \vec{p}_{A,B}^î||=||\Delta \vec{p}_{C,D}^î|| \Leftrightarrow A=C \wedge B=D$.
% \end{itemize}
% Ukážeme, že volba $k=2max_{i \in N}||\Delta \vec{p}_{A,B}^î||_2$ tyto podmínky splňuje. První vlastnost je splněna triviálně z trojúhelníkové nerovnosti v definici normy. Bodu 2 by odpovídalo vydělení konstantou (vzhledem k volbě sloupce). Tomu naše volba odpovídá.
% 
% Neuronové síti dáme velké množství dat $\Delta \vec{p}_{A,B}^1, \Delta \vec{p}_{A,B}^,...,\Delta \vec{p}_{A,B}^N)$ a jim odpovídající $ d(\vec{r}_{res,A},\vec{r}_{res,B})$, která získáme napočítáním všech dvojic z dat z velkého množství dříve napočtených vsázek. Neuronová síť si pak vytvoří model z $N$ funkcionálů $\phi^1, \phi^2,...,\phi^N$ takových, že ze vstupu $\Delta \vec{p}_{A,B}^1, \Delta \vec{p}_{A,B}^,...,\Delta \vec{p}_{A,B}^N)$ vypočtou $d(\vec{r}_{res,A},\vec{r}_{res,B})$. Využíváme velké množství dat, protože z povahy neuronových sítí plyne, že s každý dalším trénovacím prvkem je síť přesnější. 
% 
% Po dostatečném natrénování sítě provedeme její validaci. Ta bude spočívat v tom, že jako vstup budeme dávat vzdálenosti vsázek, které nebyly v tréninkové množině, a budeme hodnotit, jak se odhad neutronové sítě liší od skutečného $d(\vec{r}_{res,A},\vec{r}_{res,B})$ vypočteného neutronickým kódem. Jesliže pak průměrná odchylka odhadu $\overline{\Delta \varphi} := d*(\vec{r}_{res,A},\vec{r}_{res,B}) - d(\vec{r}_{res,A},\vec{r}_{res,B})$, kde $d*(\vec{r}_{res,A},\vec{r}_{res,B})$ je odhad neuronové sítě, bude menší než empiricky stanovená prahov hodnota úspěšnosti $\overline{\Delta \varphi}_{max}$, budeme mít dobrý odhad vzdálenosti v $\Omega_{res}$ na základě znalosti vzdálenosti z $\Omega_{LP}$. Z funkcionálů $\phi^1, \phi^2,...,\phi^N$ pak můžeme určit souvislost mezi vzdálenostmi na těch 2 prostorech.
% 
% %Pak pomocí ANN budeme hledat $N$tici funkcionálů $\phi^1, \phi^2,...,\phi^N$, kde $\forall i \in N: \sum_{i=1}^N \phi^i(\Delta \vec{p}_{A,B}^i) = d(\vec{r}_{res,A},\vec{r}_{res,B})$. 
% %Za účelem hledání těchto funkcionálů vezmeme data o velkém množství vsázek\footnote{Využijeme vlastnosti neuronových sítí, že jejich } a pro všechny dvojice $(\mathbb{A},\mathbb{B}$ z $\Omega_{LP}$ napočítáme $\Delta \vec{p}_{A,B}^1, \Delta \vec{p}_{A,B}^,...,\Delta \vec{p}_{A,B}^N)$ a jim odpovídající $ d(\vec{r}_{res,A},\vec{r}_{res,B})$. 
% 
% %Takto připravená data pak předáme neuronové síti. Najde-li dostatečně přesně $N$tici funkcionálů, která dobře aproximuje funkci $f: d(\mathbb{A},\mathbb{B})\rightarrow d(\vec{r}_{res,A},\vec{r}_{res,B})$. Tedy bychom dostali velmi přesnou informaci, jak souvisí vzdálenosti na $\Omega_{LP}$ se vzdálenostmi v $\Omega_{res}$.
% 
% 
% 
% Nutno tam zmínit, že provádět jen tak bezmyšlenkovitě $\Delta p_{A,B}^i$ je fyzikálně špatně, neboť soubory na jednotlivých pozicích nejsou nezávislé, nýbrž jsou ovlivněny svým okolím (ve smyslu souborů na okolních pozicích v AZ) - pro příklad uveďme situaci, kdy má jistý soubor v AZ vedle sebe čerstvý soubor, generující velké množství neutronů, což obecně bude zvyšovat množství probíhajících reakcí (ve srovnání se situací, kdy by vedle byl soubor s velkým vyhořením). Pro začátek předpokládejme, že to neuronová síť zvládne a vrátí vyhovující funkcionály.
% 
% Všude, kde jsme užívali $||.||_2$, byla tato volba neodůvodněná a obecně špatně. Šlo pouze o naše zjednodušení, abychom zjednodušili výpočty. Pro korektnost výpočtu bychom tam měli hledat obecné metriky, které budou zlepšovat výsledky. Obecné metriky by však mohly být nevhodné k reprezentaci v počítači. Proto se omezme na třídu vážených $p$-metrik definovaných jako
% \begin{equation}
% 	d(\vec{a}, \vec{b}) = \sqrt[p]{\vec{(a-b)^{.p}}^T \mathbb{W} \vec{(a-b)^{.p}} },
% \end{equation}
% kde $p>1$, $\mathbb{W}$ je pozitivně definitní symetrická matice, vektory uvažujeme sloupcové a $a^{.p}$ značí vektor umocněný po složkách (element-wise), tedy vznikl umocněním všech složek původního vektoru\footnote{úlohu jsme takto původně neřešili, protože nám vzrůstá počet vnořených cyklů a náročnost úlohy rychle roste.}.
% 
% \section{Metody ANN obecně}
% Metody ANN jsou založeny na postupném zpřesňování modelu mezi vstupy a výstupy. Jsou navrženy, aby "fitnuly téměř cokoliv". Probíhají ve 3 krocích:
% \begin{enumerate}
%     \item učení - v této fázi algoritmu předkládáme známé dvojice vstup-výstup (např. vzor-obraz při neznámém zobrazení, prvek z množiny (a jeho vlastnosti) - a jeho klasifikace,...). Čím větší množství dat k učení ANN dáme, tím lépe by měla dané zobrazené najít.
%     \item validace
%     \item testování - model získaný učením testujeme na vstupy (jejichž výstupy neznáme). Poté porovnáme výsledky neutronové síte a skutečná výstupní data. 
% \end{enumerate}
% 1) u
% 
% \section{Metody ANN - implementace v MATLABu}
% Implementace metod ANN v MATLABu zahrnuje metody pro aproximace funkcí (to chceme), rozpoznání obrazu, predikce vývoje a další.
% 
% Volíme aproximaci funkcí. Vstupem je 1 matice $R^{mn}$, kde $n$ je počet vstupních dat a $m$ počet parametrů příslušejících každému vstupu. V našem případě by $n$ odpovídalo počtu dvojic vsázek, $m$ počtu $\Delta p_{A,B}^i$, tj. počtu pozic v AZ. Výstupní matice bude typu $R^{1,n}$, tj. řádkovému vektoru, jehož $j$-tý prvek odpovídá vzdálenosti odezev 2 vsázek. 
% 
% %https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf
% 
% 
% 
% V MATLABu je nejjednodušší metodou metoda Shallow Neural Network.
% 
% 
% 
